## Advanced

## Non-interactive / CI mode

Run tunacode head-less in pipelines. Example GitHub Action step:

```yaml
- name: Update changelog via tunacode
  run: |
    npm install -g @openai/tunacode
    tunacode login --api-key "${{ secrets.OPENAI_KEY }}"
    tunacode exec --full-auto "update CHANGELOG for next release"
```

### Resuming non-interactive sessions

You can resume a previous headless run to continue the same conversation context and append to the same rollout file.

Interactive TUI equivalent:

```shell
tunacode resume             # picker
tunacode resume --last      # most recent
tunacode resume <SESSION_ID>
```

Compatibility:

- Latest source builds include `tunacode exec resume` (examples below).
- Current released CLI may not include this yet. If `tunacode exec --help` shows no `resume`, use the workaround in the next subsection.

```shell
# Resume the most recent recorded session and run with a new prompt (source builds)
tunacode exec "ship a release draft changelog" resume --last

# Alternatively, pass the prompt via stdin (source builds)
# Note: omit the trailing '-' to avoid it being parsed as a SESSION_ID
echo "ship a release draft changelog" | tunacode exec resume --last

# Or resume a specific session by id (UUID) (source builds)
tunacode exec resume 7f9f9a2e-1b3c-4c7a-9b0e-123456789abc "continue the task"
```

Notes:

- When using `--last`, tunacode picks the newest recorded session; if none exist, it behaves like starting fresh.
- Resuming appends new events to the existing session file and maintains the same conversation id.

## Tracing / verbose logging

Because tunacode is written in Rust, it honors the `RUST_LOG` environment variable to configure its logging behavior.

The TUI defaults to `RUST_LOG=tunacode_core=info,tunacode_tui=info` and log messages are written to `~/.tunacode/log/tunacode-tui.log`, so you can leave the following running in a separate terminal to monitor log messages as they are written:

```
tail -F ~/.tunacode/log/tunacode-tui.log
```

By comparison, the non-interactive mode (`tunacode exec`) defaults to `RUST_LOG=error`, but messages are printed inline, so there is no need to monitor a separate file.

See the Rust documentation on [`RUST_LOG`](https://docs.rs/env_logger/latest/env_logger/#enabling-logging) for more information on the configuration options.

## Model Context Protocol (MCP)

The tunacode CLI can be configured to leverage MCP servers by defining an [`mcp_servers`](./config.md#mcp_servers) section in `~/.tunacode/config.toml`. It is intended to mirror how tools such as Claude and Cursor define `mcpServers` in their respective JSON config files, though the tunacode format is slightly different since it uses TOML rather than JSON, e.g.:

```toml
# IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.
[mcp_servers.server-name]
command = "npx"
args = ["-y", "mcp-server"]
env = { "API_KEY" = "value" }
```

## Using tunacode as an MCP Server

The tunacode CLI can also be run as an MCP _server_ via `tunacode mcp-server`. For example, you can use `tunacode mcp-server` to make tunacode available as a tool inside of a multi-agent framework like the OpenAI [Agents SDK](https://platform.openai.com/docs/guides/agents). Use `tunacode mcp` separately to add/list/get/remove MCP server launchers in your configuration.

### tunacode MCP Server Quickstart
You can launch a tunacode MCP server with the [Model Context Protocol Inspector](https://modelcontextprotocol.io/legacy/tools/inspector):

``` bash
npx @modelcontextprotocol/inspector tunacode mcp-server
```
Send a `tools/list` request and you will see that there are two tools available:

**`tunacode`** - Run a tunacode session. Accepts configuration parameters matching the tunacode Config struct. The `tunacode` tool takes the following properties:

Property           | Type     | Description
-------------------|----------|----------------------------------------------------------------------------------------------------------
**`prompt`** (required)             | string   | The initial user prompt to start the tunacode conversation.
`approval-policy`    | string   | Approval policy for shell commands generated by the model: `untrusted`, `on-failure`, `never`.
`base-instructions`  | string   | The set of instructions to use instead of the default ones.
`config`             | object   | Individual [config settings](https://github.com/openai/tunacode/blob/main/docs/config.md#config) that will override what is in `$tunacode_HOME/config.toml`.
`cwd`                | string   | Working directory for the session. If relative, resolved against the server process's current directory.
`include-plan-tool`  | boolean  | Whether to include the plan tool in the conversation.
`model`             | string   | Optional override for the model name (e.g. `o3`, `o4-mini`).
`profile`            | string   | Configuration profile from `config.toml` to specify default options.
`sandbox`           | string   | Sandbox mode: `read-only`, `workspace-write`, or `danger-full-access`.

**`tunacode-reply`** - Continue a tunacode session by providing the conversation id and prompt. The `tunacode-reply` tool takes the following properties:

Property   | Type   | Description
-----------|--------|---------------------------------------------------------------
**`prompt`** (required)     | string | The next user prompt to continue the tunacode conversation.
**`conversationId`** (required)  | string | The id of the conversation to continue.

### Trying it Out
> [!TIP]
> tunacode often takes a few minutes to run. To accommodate this, adjust the MCP inspector's Request and Total timeouts to 600000ms (10 minutes) under â›­ Configuration.

Use the MCP inspector and `tunacode mcp-server` to build a simple tic-tac-toe game with the following settings:

**approval-policy:** never

**prompt:** Implement a simple tic-tac-toe game with HTML, Javascript, and CSS. Write the game in a single file called index.html.

**sandbox:** workspace-write

Click "Run Tool" and you should see a list of events emitted from the tunacode MCP server as it builds the game.
